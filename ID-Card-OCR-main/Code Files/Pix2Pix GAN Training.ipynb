{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97661538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.optim import lr_scheduler\n",
    "import functools\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2ae067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(directory):\n",
    "    \n",
    "    images = []\n",
    "    assert os.path.isdir(directory), '%s is not a valid directory' % directory\n",
    "\n",
    "    for root, _, fnames in sorted(os.walk(directory)):\n",
    "        for fname in fnames:\n",
    "            path = os.path.join(root, fname)\n",
    "            images.append(path)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68906f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __make_power_2(img, base, method=Image.BICUBIC):\n",
    "    \n",
    "    ow, oh = img.size\n",
    "    h = int(round(oh / base) * base)\n",
    "    w = int(round(ow / base) * base)\n",
    "    \n",
    "    if h == oh and w == ow:\n",
    "        return img\n",
    "\n",
    "    __print_size_warning(ow, oh, w, h)\n",
    "    \n",
    "    return img.resize((w, h), method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa81eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __print_size_warning(ow, oh, w, h):\n",
    "    \n",
    "    if not hasattr(__print_size_warning, 'has_printed'):\n",
    "        \n",
    "        print(\"The image size needs to be a multiple of 4. \"\n",
    "              \"The loaded image size was (%d, %d), so it was adjusted to \"\n",
    "              \"(%d, %d). This adjustment will be done to all images \"\n",
    "              \"whose sizes are not multiples of 4\" % (ow, oh, w, h))\n",
    "        \n",
    "        __print_size_warning.has_printed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7efc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(method):\n",
    "    \n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n",
    "\n",
    "    transform_list += [transforms.ToTensor()]\n",
    "    transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "            \n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55ee880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(net, gpu_ids, init_type='normal', init_gain=0.02):\n",
    "\n",
    "#     if len(gpu_ids) > 0:\n",
    "    if gpu_ids[0] != -1:\n",
    "        \n",
    "        assert(torch.cuda.is_available())\n",
    "        net.to(gpu_ids[0])\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n",
    "    \n",
    "    def init_func(m): \n",
    "        \n",
    "        classname = m.__class__.__name__\n",
    "        \n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            \n",
    "            init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            \n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                \n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "                \n",
    "        elif classname.find('BatchNorm2d') != -1: \n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    net.apply(init_func)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e024a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, epoch_count, n_epochs, n_epochs_decay):\n",
    "\n",
    "    def lambda_rule(epoch):\n",
    "        lr_l = 1.0 - max(0, epoch + epoch_count - n_epochs) / float(n_epochs_decay + 1)\n",
    "        return lr_l\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f392da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_requires_grad(nets, requires_grad=False):\n",
    "    \n",
    "        if not isinstance(nets, list):\n",
    "            nets = [nets]\n",
    "            \n",
    "        for net in nets:\n",
    "            \n",
    "            if net is not None:\n",
    "                \n",
    "                for param in net.parameters():\n",
    "                    \n",
    "                    param.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149d6d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "       \n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        \n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "        \n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "       \n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247be7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=True, n_blocks=9, padding_type='reflect'):\n",
    "\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        \n",
    "        use_bias=False\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):  # add upsampling layers\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e964886",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        \n",
    "        use_bias=False\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "      \n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2453730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "   \n",
    "    def __init__(self, gan_mode='lsgan', target_real_label=1.0, target_fake_label=0.0):\n",
    " \n",
    "        super(GANLoss, self).__init__()\n",
    "    \n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        \n",
    "        self.gan_mode = gan_mode\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "       \n",
    "        target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "        loss = self.loss(prediction, target_tensor)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f003f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataroot, phase):\n",
    "       \n",
    "        self.dataroot = dataroot\n",
    "        self.phase = phase\n",
    "        self.dir_AB = os.path.join(dataroot, phase)  # get the image directory\n",
    "        self.AB_paths = sorted(make_dataset(self.dir_AB))  # get image paths\n",
    "                               \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # read a image given a random integer index\n",
    "        AB_path = self.AB_paths[index]\n",
    "        AB = Image.open(AB_path).convert('RGB')\n",
    "                               \n",
    "        # split AB image into A and B AFTER HAVING BEEN PROCESSED FROM THE SCRIPT \"combine_A_and_B.py\"\n",
    "        w, h = AB.size\n",
    "        w2 = int(w / 2)\n",
    "        A = AB.crop((0, 0, w2, h))\n",
    "        B = AB.crop((w2, 0, w, h))\n",
    "\n",
    "        # apply the same transform to both A and B\n",
    "        A_transform = get_transform(Image.BICUBIC)\n",
    "        B_transform = get_transform(Image.BICUBIC)\n",
    "\n",
    "        A = A_transform(A)\n",
    "        B = B_transform(B)\n",
    "\n",
    "        return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}\n",
    "\n",
    "    def __len__(self):\n",
    "       \n",
    "        return len(self.AB_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64614a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetDataLoader():\n",
    "\n",
    "    def __init__(self, dataroot, phase):\n",
    "        \n",
    "        self.dataroot = dataroot\n",
    "        self.phase = phase\n",
    "        self.dataset = AlignedDataset(dataroot, phase)\n",
    "        \n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=0)\n",
    "\n",
    "    def load_data(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        for i, data in enumerate(self.dataloader):\n",
    "            yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82952cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixModel:\n",
    "\n",
    "    def __init__(self, gpu_ids):\n",
    "        \n",
    "        self.gpu_ids = gpu_ids\n",
    "        self.device = torch.device('cuda:{}'.format(gpu_ids[0])) if gpu_ids[0]!=-1 else torch.device('cpu')\n",
    "\n",
    "        self.optimizers = []\n",
    "        \n",
    "        self.netG = ResnetGenerator(input_nc=3, output_nc=3, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=True, n_blocks=9)\n",
    "        init_net(self.netG, gpu_ids, 'normal', 0.02)\n",
    "        \n",
    "        self.netD = NLayerDiscriminator(input_nc=6, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d)\n",
    "        init_net(self.netD, gpu_ids, 'normal', 0.02)\n",
    "\n",
    "        self.criterionGAN = GANLoss(gan_mode='lsgan').to(self.device)\n",
    "        self.criterionL1 = torch.nn.L1Loss()\n",
    "\n",
    "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.optimizers.append(self.optimizer_G)\n",
    "        self.optimizers.append(self.optimizer_D)\n",
    "\n",
    "    def set_input(self, input):\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "        self.real_A = input['A'].to(self.device)\n",
    "#         self.real_A = torch.unsqueeze(self.real_A, dim=0)\n",
    "\n",
    "        self.real_B = input['B'].to(self.device)\n",
    "#         self.real_B = torch.unsqueeze(self.real_B, dim=0)\n",
    "\n",
    "        self.image_paths = input['A_paths']\n",
    "\n",
    "    def forward(self):\n",
    "  \n",
    "        self.fake_B = self.netG(self.real_A)\n",
    "\n",
    "    def backward_D(self):\n",
    "        \n",
    "        fake_AB = torch.cat((self.real_A, self.fake_B), 1) \n",
    "        pred_fake = self.netD(fake_AB.detach())\n",
    "        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "        \n",
    "        real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
    "        pred_real = self.netD(real_AB)\n",
    "        \n",
    "        self.loss_D_real = self.criterionGAN(pred_real, True)\n",
    "        \n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "        \n",
    "        return (self.loss_D_real, self.loss_D_fake)\n",
    "\n",
    "    def backward_G(self):\n",
    "\n",
    "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
    "        pred_fake = self.netD(fake_AB)\n",
    "        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
    "        \n",
    "        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * 100.0\n",
    "        \n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "        \n",
    "        return (self.loss_G_GAN, self.loss_G_L1)\n",
    "\n",
    "    def optimize_parameters(self):\n",
    "        \n",
    "        self.forward()              \n",
    "        \n",
    "        set_requires_grad(nets=self.netD, requires_grad=True)       \n",
    "        self.optimizer_D.zero_grad()     \n",
    "        rf_losses = self.backward_D()              \n",
    "        self.optimizer_D.step()    \n",
    "        \n",
    "        set_requires_grad(nets=self.netD, requires_grad=False)  \n",
    "        self.optimizer_G.zero_grad()        \n",
    "        gl_losses = self.backward_G()                 \n",
    "        self.optimizer_G.step() \n",
    "        \n",
    "        return (rf_losses, gl_losses, self.optimizer_G, self.optimizer_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35633acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining dataset object\n",
    "# dataset = AlignedDataset('C:/Users/muhammad.ispahani/Desktop/Train_Dir', 'train')\n",
    "\n",
    "# # Defining the Dataloader\n",
    "# data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86bd0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = CustomDatasetDataLoader('D:/Data Science Projects/CNIC OCR/GANs/pytorch-CycleGAN-and-pix2pix-master/datasets/cnictopix', 'train')\n",
    "dataset = data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49c4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of Model\n",
    "\n",
    "gpu_ids = [-1]\n",
    "pix2pix = Pix2PixModel(gpu_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed7c04ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count = 1\n",
    "n_epochs = 1\n",
    "n_epochs_decay = 1\n",
    "\n",
    "schedulers = [get_scheduler(optimizer, epoch_count, n_epochs, n_epochs_decay) for optimizer in pix2pix.optimizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b20ab3b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number on Training:  1 \n",
      "\n",
      "Losses for Epoch number  1  for iteration  1  are G_GAN: 3.2179808616638184 , G_L1: 64.8541259765625 , D_real: 1.1071852445602417 , D_fake: 1.2025845050811768 \n",
      "\n",
      "Losses for Epoch number  1  for iteration  2  are G_GAN: 18.001569747924805 , G_L1: 62.300289154052734 , D_real: 3.369034767150879 , D_fake: 3.060173749923706 \n",
      "\n",
      "Losses for Epoch number  1  for iteration  3  are G_GAN: 66.40013122558594 , G_L1: 54.997802734375 , D_real: 15.453411102294922 , D_fake: 17.869972229003906 \n",
      "\n",
      "Epoch Number on Training:  2 \n",
      "\n",
      "Losses for Epoch number  2  for iteration  1  are G_GAN: 245.7174072265625 , G_L1: 89.74800109863281 , D_real: 55.386600494384766 , D_fake: 69.54033660888672 \n",
      "\n",
      "Losses for Epoch number  2  for iteration  2  are G_GAN: 411.45989990234375 , G_L1: 88.44414520263672 , D_real: 250.62472534179688 , D_fake: 265.6634521484375 \n",
      "\n",
      "Losses for Epoch number  2  for iteration  3  are G_GAN: 512.1514282226562 , G_L1: 93.44236755371094 , D_real: 380.9671936035156 , D_fake: 408.4616394042969 \n",
      "\n",
      "Total time taken for training data on  2 s is:  93.84134912490845\n"
     ]
    }
   ],
   "source": [
    "total_iters = 0\n",
    "\n",
    "for epoch in range(epoch_count, n_epochs + n_epochs_decay + 1):  \n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    " \n",
    "    epoch_iter = 0\n",
    "    \n",
    "    print(\"Epoch Number on Training: \", epoch, '\\n')\n",
    "\n",
    "    old_lr = pix2pix.optimizers[0].param_groups[0]['lr']\n",
    "    \n",
    "    for scheduler in schedulers:\n",
    "        scheduler.step()\n",
    "        \n",
    "    lr = pix2pix.optimizers[0].param_groups[0]['lr']\n",
    "    \n",
    "    for i, data in enumerate(dataset):  \n",
    "\n",
    "        total_iters += 1\n",
    "        epoch_iter += 1\n",
    "        \n",
    "        pix2pix.set_input(data) \n",
    "        \n",
    "        (rf_losses, gl_losses, opt_G, opt_D) = pix2pix.optimize_parameters() \n",
    "        \n",
    "#         if total_iters % 10 == 0: \n",
    "        \n",
    "        losses = [float(gl_losses[0].item()), float(gl_losses[1].item()), float(rf_losses[0].item()), float(rf_losses[1].item())]\n",
    "\n",
    "        print('Losses for Epoch number ', epoch, ' for iteration ', i+1 , ' are G_GAN:', losses[0], ', G_L1:', losses[1],\\\n",
    "              ', D_real:', losses[2], ', D_fake:', losses[3], '\\n')\n",
    "            \n",
    "    # Saving Model at each epoch\n",
    "    \n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_G': pix2pix.netG.state_dict(),\n",
    "                'model_D': pix2pix.netD.state_dict(),\n",
    "                'optimizer_G_state_dict': opt_G.state_dict(),\n",
    "                'optimizer_D_state_dict': opt_D.state_dict(),\n",
    "                'loss_G_GAN': float(gl_losses[0].item()),\n",
    "                'loss_G_L1': float(gl_losses[1].item()),\n",
    "                'loss_D_real': float(rf_losses[0].item()),\n",
    "                'loss_D_fake': float(rf_losses[0].item())}, 'model.pth')\n",
    "#     torch.save({'epoch': epoch,\n",
    "#                 'model': pix2pix.netG.state_dict(),\n",
    "#                 'optimizer': opt_G.state_dict()}, 'model.pth')\n",
    "    \n",
    "print('Total time taken for training data on ', epoch, 's is: ', time.time() - epoch_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946e812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f9b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
